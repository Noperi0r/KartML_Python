{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd # Data Processing \n",
    "from sklearn.model_selection import train_test_split # Categorize trainset, validation set \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset \n",
    "\n",
    "import glob # File to list\n",
    "from tqdm import tqdm # Process bar \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.hidden1 = nn.Linear(hidden_dim, hidden_dim) \n",
    "        self.leaky_relu1 = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.glu = nn.GLU() # Gated Linear Unit splits feature into half >> ex 128 to 64.\n",
    "\n",
    "        self.hidden3 = nn.Linear(hidden_dim//2, hidden_dim)\n",
    "        self.leaky_relu3 = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        self.hidden4 = nn.Linear(hidden_dim, hidden_dim) \n",
    "        self.glu2 = nn.GLU() # half\n",
    "\n",
    "        self.hidden5 = nn.Linear(hidden_dim//2, input_dim) \n",
    "        self.leaky_relu5 = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        # 32 x 64 / 128 x 64\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.leaky_relu1(self.hidden1(x))\n",
    "        out = self.glu(self.hidden2(out))\n",
    "        out = self.leaky_relu3(self.hidden3(out))\n",
    "        out = self.glu2(self.hidden4(out))\n",
    "        out = self.leaky_relu5(self.hidden5(out))\n",
    "        return out\n",
    "\n",
    "class MovePredictionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MovePredictionModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        #self.leaky_relu = nn.LeakyReLU(negative_slope=0.3)\n",
    "\n",
    "        # Residual block\n",
    "        self.residual_block = ResidualBlock(input_dim, hidden_dim)\n",
    "\n",
    "        # self.hidden_skiplayer = nn.Linear(hidden_dim//2, input_dim)\n",
    "        self.hidden_skiplayer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.leakyrelu_skiplayer = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        #self.hidden = nn.Linear(hidden_dim, output_dim)\n",
    "        #self.leakyrelu = nn.LeakyReLU(negative_slope=0.02)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #processed_input = self.leaky_relu(self.input_layer(x))\n",
    "        processed_input = self.input_layer(x)\n",
    "        residual_out = self.residual_block(processed_input) # output tensor size: input_dim\n",
    "        \n",
    "        out = x + residual_out\n",
    "        out = self.hidden_skiplayer(out)\n",
    "        out = self.leakyrelu_skiplayer(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = 9 # Linear vel + Linear acc + Angular vel + Angular acc + pos diff(vel*delta t) + rot diff(angvel * delta t) + bIsDrifting\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 3 # Pos diff x y  and rot diff z \n",
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 32 # TODO\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "OUTPUT_FILENAME = 'model/test.pth'\n",
    "DATA_PATH = 'data'\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(cuda_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5164, 0.7387, 0.5978,  ..., 0.5164, 0.5000, 0.5908],\n",
      "        [0.5205, 0.5191, 0.5081,  ..., 0.5205, 0.5000, 0.6030],\n",
      "        [0.5234, 0.5209, 0.5042,  ..., 0.5234, 0.4958, 0.6161],\n",
      "        ...,\n",
      "        [0.4160, 0.5927, 0.5218,  ..., 0.4160, 0.5007, 0.2357],\n",
      "        [0.4154, 0.5180, 0.4932,  ..., 0.4154, 0.5042, 0.2297],\n",
      "        [0.4032, 0.4501, 0.4555,  ..., 0.4032, 0.5042, 0.2217]])\n",
      "torch.Size([32, 9])\n"
     ]
    }
   ],
   "source": [
    "# Read .csv files with pandas\n",
    "file_path = \"data/data0.csv\"\n",
    "data = pd.read_csv(file_path) # data frame \n",
    "\n",
    "# csv_file_paths = glob.glob(f'{DATA_PATH}/**/*.csv', recursive=True)\n",
    "# for path in csv_file_paths:\n",
    "#     print(path)\n",
    "\n",
    "# Convert to tensor \n",
    "x_features_csv = [i for i in range(9)] # 0 - 8 idx\n",
    "y_features_csv = [i for i in range(9, 12)] # 9 - 11 idx \n",
    "\n",
    "input_features = data.values[:,x_features_csv] # 0 - 8\n",
    "output_features = data.values[:, y_features_csv]\n",
    "\n",
    "# Min max scaling\n",
    "x_min, x_max = input_features.min(axis=0), input_features.max(axis=0)\n",
    "y_min, y_max = output_features.min(axis=0), output_features.max(axis=0)\n",
    "\n",
    "input_features_scaled = (input_features - x_min) / (x_max - x_min)\n",
    "output_features_scaled = (output_features - y_min) / (y_max - y_min)\n",
    "\n",
    "X = torch.tensor(input_features_scaled, dtype=torch.float32)\n",
    "y = torch.tensor(output_features_scaled, dtype=torch.float32)\n",
    "\n",
    "print(X)\n",
    "#print(X.shape, y.shape)\n",
    "\n",
    "# Dataset split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# print(f'Training samples: {len(train_loader.dataset)}')\n",
    "# print(f'Validation samples: {len(val_loader.dataset)}')\n",
    "\n",
    "batch = next(iter(train_loader))  # 첫 번째 배치 가져오기\n",
    "print(batch[0].shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation RMSE: nan\n",
      "Epoch 3, Validation RMSE: nan\n",
      "Epoch 5, Validation RMSE: nan\n",
      "Epoch 7, Validation RMSE: nan\n",
      "Epoch 9, Validation RMSE: nan\n",
      "Epoch 11, Validation RMSE: nan\n",
      "Epoch 13, Validation RMSE: nan\n",
      "Epoch 15, Validation RMSE: nan\n",
      "Epoch 17, Validation RMSE: nan\n",
      "Epoch 19, Validation RMSE: nan\n",
      "Epoch 21, Validation RMSE: nan\n",
      "Epoch 23, Validation RMSE: nan\n",
      "Epoch 25, Validation RMSE: nan\n",
      "Epoch 27, Validation RMSE: nan\n",
      "Epoch 29, Validation RMSE: nan\n",
      "Epoch 31, Validation RMSE: nan\n",
      "Epoch 33, Validation RMSE: nan\n",
      "Epoch 35, Validation RMSE: nan\n",
      "Epoch 37, Validation RMSE: nan\n",
      "Epoch 39, Validation RMSE: nan\n",
      "Epoch 41, Validation RMSE: nan\n",
      "Epoch 43, Validation RMSE: nan\n",
      "Epoch 45, Validation RMSE: nan\n",
      "Epoch 47, Validation RMSE: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_batch)\n\u001b[1;32m---> 26\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m epochs\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\01076\\anaconda3\\envs\\lstm_test\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\01076\\anaconda3\\envs\\lstm_test\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\01076\\anaconda3\\envs\\lstm_test\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set model\n",
    "model = MovePredictionModel(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "#model = ResidualBlock(INPUT_DIM, HIDDEN_DIM)\n",
    "if cuda_available:\n",
    "    model.cuda()\n",
    "\n",
    "epochs = EPOCHS\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# TODO: Early stopping\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Tranining \n",
    "    model.train()\n",
    "    #for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} Training\"):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        if cuda_available:\n",
    "            X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 2 == 0 or epoch == epochs-1:\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        val_rmse = []\n",
    "        y_preds = []\n",
    "        y_actuals = []\n",
    "        with torch.no_grad():\n",
    "            #for X_batch, y_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} Validation\"):\n",
    "            for X_batch, y_batch in val_loader:\n",
    "\n",
    "                ## if you have GPU\n",
    "                if cuda_available:\n",
    "                    X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "\n",
    "                # inference the model\n",
    "                y_pred = model(X_batch)\n",
    "\n",
    "                # calculate RMSE\n",
    "                rmse = torch.sqrt(criterion(y_pred, y_batch)).cpu().numpy()\n",
    "                val_rmse.append(rmse)\n",
    "\n",
    "                # for the first batch\n",
    "                if len(y_preds) == 0:  \n",
    "                    y_preds = y_pred.cpu().numpy()\n",
    "                    y_actuals = y_batch.cpu().numpy()\n",
    "                # for the rest of the batches\n",
    "                else:  \n",
    "                    y_preds = np.vstack((y_preds, y_pred.cpu().numpy()))\n",
    "                    y_actuals = np.vstack((y_actuals, y_batch.cpu().numpy()))\n",
    "        epoch_val_rmse = np.mean(val_rmse)\n",
    "        print(f\"Epoch {epoch+1}, Validation RMSE: {epoch_val_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
