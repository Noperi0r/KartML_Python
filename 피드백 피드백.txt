1. relu > input으로 difference 이전꺼 가져오기


2. min max scaling > 쓸거면 inference 시에도  normalization ★
학습, inference 조건 동일.
batch normalization > 데이터값 기반 해서 각각씩 정규화 >> inference 시에 값 한개만 전달해주면 불일치 가능성
데이터 10만 20만 있으면 모르겠는데 배치 안의 max min이 전체 표현하지 않을 가능성 높음
inference도 스케일링
feature 한 데이터셋에서 min max X
각 dim마다 min max  scaling해야됨 안그러면 정보손실 ★


가중치 


- 레포트 residual 빼고 안 빼고

마지막 activation relu x.
앞으로 많이가고 뒤로적게갈때는 써도 되는 그런거 성질 좀 고려 필요
sigmoid에서 + - 가 나올 가능성 등 고려해서

- 순서가 필요 없음 > inference 시에 data 한개만 들어감.  > shuffle 가능
데이터가 overfitting될거라고 사람이 생각하면 무조건 난다 
shuffle 하고안하고 비교

- lstm 쓸거면  shuffle >> sequence 단위로 모아서 그 단위끼리 shuffle

- validation 시 shuffle x.  >> 실제환경과 유사해야됨

- loss weight 주는게 맞나? > ㅇ
- 0.1초 0.1초 데이터 // infer 시에도 0.1 0.1? >> vel mul delta 등 deltatime 포함된 input feature 있어서 delta 0.1에서 0.0001로 바뀌거나 하면 input도 바뀔거라 안됡임

- 데이터 많을수록 좋긴한데 overfit 가능성